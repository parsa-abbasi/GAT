# GAT
A repository containing all about the Graph Attention Networks  (GATs).

![](https://camo.githubusercontent.com/4fe1a90e67d17a2330d7cfcddc930d5f7501750c/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f71327a703170366b37396a6a6431352f6761745f6c617965722e706e673f7261773d31)

## Papers

The original paper introducing the architecture of the GATs:

[	Graph Attention Networks (Veličković *et al.*, ICLR 2018)](https://arxiv.org/abs/1710.10903)

There is a recently published paper that provides a new version of this architecture (GATv2) with dynamic attention.

​	[How Attentive are Graph Attention Networks? (Brody et al., arXiv 2021)](https://arxiv.org/abs/2105.14491)

## Presentations

I recently introduced the original GAT architecture to my coursemates in the Deep Learning course (spring 2021) at IUST. The presentation file of this talk is available in the repository.

## Other sources

**Videos:**

- [Understanding Graph Attention Networks by DeepFindr](https://youtu.be/A-yKQamf2Fc)

- [Graph Attention Networks (GAT) | GNN Paper Explained by The AI Epiphany](https://youtu.be/uFLeKkXWq2c)

- [Graph Attention Network Project Walkthrough by The AI Epiphany](https://youtu.be/364hpoRB4PQ)

**Codes:**

- [GAT - Original implementation by Petar Veličković](https://github.com/PetarV-/GAT)
- [How Attentive are Graph Attention Networks?](https://github.com/tech-srl/how_attentive_are_gats)

- [GAT - Graph Attention Network (PyTorch) by Aleksa Gordic](https://github.com/gordicaleksa/pytorch-GAT.git)

**Blog posts:**

- [Graph Attention Networks Under the Hood by Giuseppe Futia](https://towardsdatascience.com/graph-attention-networks-under-the-hood-3bd70dc7a87)

